from math import ceil
from multiprocessing.context import Process
import os
import bz2
import pickle
from typing import List
from numpy import savez_compressed, load
from pandas.core.frame import DataFrame
from create_shadow_models import (
    HOMEDIR,
    N_MODELS,
    N_WORKERS,
    TEST_SIZE,
    create_random_forest,
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from pandas import read_pickle, concat
from imblearn.under_sampling import RandomUnderSampler

BLACK_BOX_TYPE = "nn"

SHADOW_TYPE = "rf"


def merge_partial_attack_dataset(under: float = None) -> List:
    """Merges different attacks generated by different shadow models.

    Args:
        under (float, optional): Undersampling value. Defaults to None.

    Returns:
        List: [description]
    """
    # Concatenates the prediction sets of different shadow models
    attack_dataset: DataFrame = concat(
        [
            read_pickle(
                f"{HOMEDIR}/shadow/{BLACK_BOX_TYPE}_{SHADOW_TYPE}_{i}/prediction_set.pkl.bz2"
            )
            for i in range(N_MODELS)
        ]
    )
    # Optional random undersampling
    sampler: RandomUnderSampler = None
    if under:
        sampler = RandomUnderSampler(sampling_strategy=under, random_state=42)
    # List of all the labels
    labels: List = []
    # For each label in the dataset train_test_splits the relative records and saves it
    for label, df in attack_dataset.groupby("class_label", sort=False):
        labels.append(label)
        # Training records
        X = df.drop(columns=["class_label", "target_label"])
        # Target label
        y = df["target_label"]
        # Splitting for model training
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=TEST_SIZE, stratify=y, random_state=42
        )
        # If and undersampling is needed, performs it
        if under:
            X_train, y_train = sampler.fit_resample(X_train, y_train)
        # Saves data on disk
        os.makedirs(
            f"{HOMEDIR}/attack/{BLACK_BOX_TYPE}_{SHADOW_TYPE}_label_{label}",
            exist_ok=True,
        )
        savez_compressed(
            f"{HOMEDIR}/attack/{BLACK_BOX_TYPE}_{SHADOW_TYPE}_label_{label}/data.npz",
            X_train=X_train,
            y_train=y_train,
            X_test=X_test,
            y_test=y_test,
        )
    return labels


def train_attack_model(label: int):
    """Trains an attack model relative to a specific label

    Args:
        label (int): Target label.
    """
    # Reads data from disk
    loaded = load(
        f"{HOMEDIR}/attack/{BLACK_BOX_TYPE}_{SHADOW_TYPE}_label_{label}/data.npz",
        allow_pickle=True,
    )
    X_train = loaded["X_train"]
    y_train = loaded["y_train"]
    # Creates a random forest with this training data
    rf: RandomForestClassifier = create_random_forest(X_train, y_train)
    # Saves the forest on disk
    with bz2.open(
        f"{HOMEDIR}/attack/{BLACK_BOX_TYPE}_{SHADOW_TYPE}_label_{label}/model.pkl.bz2",
        "wb",
    ) as f:
        pickle.dump(rf, f)
    X_test = loaded["X_test"]
    y_test = loaded["y_test"]
    # Evaluates the attack model performances
    train_report = classification_report(y_train, rf.predict(X_train))
    test_report = classification_report(y_test, rf.predict(X_test))
    # Saves the report on disk
    with open(
        f"{HOMEDIR}/attack/{BLACK_BOX_TYPE}_{SHADOW_TYPE}_label_{label}/train_report.txt",
        "w",
    ) as f:
        f.write(train_report)
    with open(
        f"{HOMEDIR}/attack/{BLACK_BOX_TYPE}_{SHADOW_TYPE}_label_{label}/test_report.txt",
        "w",
    ) as f:
        f.write(test_report)


def worker(id: int, labels: List[int]):
    """Parallel worker that creates attack models for each element in the label

    Args:
        labels (List[int]): List of labels to compute shadow model.
    """
    print(f"Worker {id} started")
    for label in labels:
        print(f"Worker {id} - Label {label}")
        train_attack_model(label)
    print(f"Worker {id} stopped")


def main():
    # Merges the partial attack datasets to create many attack datasets
    print("Merging dataset")
    labels: List[int] = merge_partial_attack_dataset()
    # Size of the chunk of label lists that each worker will take
    chunk_size: int = ceil(len(labels) / N_WORKERS)
    # List of worker processes
    processes: List[Process] = []
    # Parallelizes shadow model training for each worker
    for i in range(N_WORKERS):
        # Begin of the range
        begin: int = i * chunk_size
        # End of the range
        end: int = min(begin + chunk_size, len(labels))
        # Labels slice
        labels_slice: List[int] = labels[begin : (end + 1)]
        process = Process(target=worker, args=(i, labels_slice))
        processes.append(process)
        process.start()
    # Joins the parallel processes
    for process in processes:
        process.join()


if __name__ == "__main__":
    main()
